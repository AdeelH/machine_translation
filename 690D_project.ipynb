{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "690D_project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdeelH/machine_translation/blob/master/690D_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTvDRIvGyhhl",
        "colab_type": "text"
      },
      "source": [
        "# Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1saQXqSwAbdw",
        "colab_type": "code",
        "outputId": "cf4fba4b-7698-488a-d788-c51a157df463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu May  9 22:56:20 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.56       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8    17W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXhJPkw8ANN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "from functools import lru_cache\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import spacy\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVG1Qn2osPaR",
        "colab_type": "text"
      },
      "source": [
        "# Data fetching and loading\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLHRY2Phu5yW",
        "colab_type": "text"
      },
      "source": [
        "Fetch from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KscuQ_HX6n1c",
        "colab_type": "code",
        "outputId": "d6439913-544b-4184-ff37-038e0aaff6fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "import pickle\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "print('success!')\n",
        "\n",
        "from google import colab\n",
        "colab.drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "success!\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTrOHFBNZozg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://drive.google.com/open?id=13bmmEylwiRzZ_-TbqspB3SmFcFu8QX3X\n",
        "train_en_file = drive.CreateFile({'id': '13bmmEylwiRzZ_-TbqspB3SmFcFu8QX3X'})\n",
        "train_en_file.GetContentFile(f'./train.en') \n",
        "\n",
        "# https://drive.google.com/open?id=11sQBdDUCAA_5idGdG1i8bKbHssvkewkQ\n",
        "train_de_file = drive.CreateFile({'id': '11sQBdDUCAA_5idGdG1i8bKbHssvkewkQ'})\n",
        "train_de_file.GetContentFile(f'./train.de') \n",
        "\n",
        "# https://drive.google.com/open?id=1iueYCDdcwfvrKjT2Z_Y7KKTI0n_FKvsW\n",
        "dev_de_file = drive.CreateFile({'id': '1iueYCDdcwfvrKjT2Z_Y7KKTI0n_FKvsW'})\n",
        "dev_de_file.GetContentFile(f'./dev.de') \n",
        "\n",
        "# https://drive.google.com/open?id=1BV_lRV3Ve88MbonusforJIYptX_KFlaB\n",
        "dev_en_file = drive.CreateFile({'id': '1BV_lRV3Ve88MbonusforJIYptX_KFlaB'})\n",
        "dev_en_file.GetContentFile(f'./dev.en') \n",
        "\n",
        "# # https://drive.google.com/open?id=1sy75H0S7G9MfSUG2EVUL8OI85CqJeUu5\n",
        "test_de_file = drive.CreateFile({'id': '1sy75H0S7G9MfSUG2EVUL8OI85CqJeUu5'})\n",
        "test_de_file.GetContentFile(f'./test.de') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfZ_jy8osbVX",
        "colab_type": "text"
      },
      "source": [
        "Load data from files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmfNKUNKANN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_en = open('train.en', encoding=\"utf8\").read().strip().split('\\n')\n",
        "train_de = open('train.de', encoding=\"utf8\").read().strip().split('\\n')\n",
        "\n",
        "dev_en = open('dev.en', encoding=\"utf8\").read().strip().split('\\n')\n",
        "dev_de = open('dev.de', encoding=\"utf8\").read().strip().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-eY9EXfsg0-",
        "colab_type": "text"
      },
      "source": [
        "Define sentence delimiters and padding and unknown tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag1hn_U5ANN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD_ = '<PAD>'\n",
        "BOS_ = '<BOS>'\n",
        "EOS_ = '<EOS>'\n",
        "UNK_ = '<unk>'\n",
        "\n",
        "PAD_IDX = 0\n",
        "BOS_IDX = 1\n",
        "EOS_IDX = 2\n",
        "UNK_IDX = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6TKQgsQsrys",
        "colab_type": "text"
      },
      "source": [
        "We use Spacy to tokenize our input sentences, so download the relevant language models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyQWX8I1As5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!spacy download en_core_web_md\n",
        "!spacy download de"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nipw6f0xs7v7",
        "colab_type": "text"
      },
      "source": [
        "Tokenize:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-ZCJ8OtANOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_en = spacy.load(\"en_core_web_md\")\n",
        "nlp_de = spacy.load(\"de\")\n",
        "\n",
        "def tokenize(sentences, tokenizer):\n",
        "    tokenized = [tokenizer(s.strip().lower()) for s in sentences]\n",
        "    tokenized = [[BOS_] + [token.text for token in sentence] + [EOS_] for sentence in tokenized]\n",
        "    return tokenized\n",
        "\n",
        "train_en_tokenized = tokenize(train_en, nlp_en.tokenizer)\n",
        "train_de_tokenized = tokenize(train_de, nlp_de.tokenizer)\n",
        "\n",
        "dev_en_tokenized = tokenize(dev_en, nlp_en.tokenizer)\n",
        "dev_de_tokenized = tokenize(dev_de, nlp_de.tokenizer)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgwbSZWVs_NG",
        "colab_type": "text"
      },
      "source": [
        "#### Populate vocabularies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn9oN6seANOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_en = {PAD_: PAD_IDX, BOS_: BOS_IDX, EOS_: EOS_IDX, UNK_: UNK_IDX}\n",
        "vocab_counts_en = {PAD_: 0, BOS_: 0, EOS_: 0, UNK_: 0}\n",
        "\n",
        "i = len(vocab_en)\n",
        "for sentence in train_en_tokenized:\n",
        "    for word in sentence:\n",
        "        if word not in vocab_en:\n",
        "            vocab_en[word] = i\n",
        "            vocab_counts_en[word] = 1\n",
        "            i += 1\n",
        "        vocab_counts_en[word] += 1\n",
        "len(vocab_en)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFdvL0p8ANOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_de = {PAD_: PAD_IDX, BOS_: BOS_IDX, EOS_: EOS_IDX, UNK_: UNK_IDX}\n",
        "vocab_counts_de = {PAD_: 0, BOS_: 0, EOS_: 0, UNK_: 0}\n",
        "\n",
        "i = len(vocab_de)\n",
        "for sentence in train_de_tokenized:\n",
        "    for word in sentence:\n",
        "        if word not in vocab_de:\n",
        "            vocab_de[word] = i\n",
        "            vocab_counts_de[word] = 1\n",
        "            i += 1\n",
        "        vocab_counts_de[word] += 1\n",
        "len(vocab_de)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1NIxJTGtJO0",
        "colab_type": "text"
      },
      "source": [
        "Save to file for faster reloading:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bb44phasVG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path = F\"/content/gdrive/My Drive/690D/vocab_de.pkl\" \n",
        "# with open(path, \"wb\") as f:\n",
        "#     pickle.dump(vocab_de, f)\n",
        "\n",
        "# path = F\"/content/gdrive/My Drive/690D/vocab_counts_de.pkl\" \n",
        "# with open(path, \"wb\") as f:\n",
        "#     pickle.dump(vocab_counts_de, f)\n",
        "\n",
        "# path = F\"/content/gdrive/My Drive/690D/vocab_en.pkl\" \n",
        "# with open(path, \"wb\") as f:\n",
        "#     pickle.dump(vocab_en, f)\n",
        "\n",
        "# path = F\"/content/gdrive/My Drive/690D/vocab_counts_en.pkl\" \n",
        "# with open(path, \"wb\") as f:\n",
        "#     pickle.dump(vocab_counts_en, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyCxqRNwtNeH",
        "colab_type": "text"
      },
      "source": [
        "Load from file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRguzoorsl-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path = F\"/content/gdrive/My Drive/690D/vocab_de.pkl\" \n",
        "# with open(path, \"rb\") as f:\n",
        "#     vocab_de = pickle.load(f)\n",
        "\n",
        "# path = F\"/content/gdrive/My Drive/690D/vocab_counts_de.pkl\" \n",
        "# with open(path, \"rb\") as f:\n",
        "#     vocab_counts_de = pickle.load(f)\n",
        "\n",
        "# path = F\"/content/gdrive/My Drive/690D/vocab_en.pkl\" \n",
        "# with open(path, \"rb\") as f:\n",
        "#     vocab_en = pickle.load(f)\n",
        "\n",
        "# path = F\"/content/gdrive/My Drive/690D/vocab_counts_en.pkl\" \n",
        "# with open(path, \"rb\") as f:\n",
        "#     vocab_counts_en = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsdjKUaetWgg",
        "colab_type": "text"
      },
      "source": [
        "#### Populate index to word mappings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18NbROqLANOK",
        "colab_type": "code",
        "outputId": "faf2e193-51f7-4cce-acc2-29cf9a031e02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "idx_to_word_en = [None] * len(vocab_en)\n",
        "for k, v in vocab_en.items():\n",
        "    idx_to_word_en[v] = k\n",
        "sum(i is None for i in idx_to_word_en)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJtT6cTgANOM",
        "colab_type": "code",
        "outputId": "9b89a8f7-c3a4-4c20-e3b7-d01d9f0c3ef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "idx_to_word_de = [None] * len(vocab_de)\n",
        "for k, v in vocab_de.items():\n",
        "    idx_to_word_de[v] = k\n",
        "sum(i is None for i in idx_to_word_de)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvy1jkRjtdg3",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWzmYjxFttu-",
        "colab_type": "text"
      },
      "source": [
        "Truncate to MAX_LEN and pad:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3s4EAwqTY2dh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuueYUOEANOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def sentence_to_inds(sentence, vocab, max_len):\n",
        "    padding = [PAD_IDX] * (max_len - len(sentence))\n",
        "    return [vocab.get(word, UNK_IDX) for word in sentence[:max_len]] + padding\n",
        "\n",
        "def sentences_to_inds(sentences, vocab, max_len):\n",
        "    return torch.LongTensor([sentence_to_inds(s, vocab, max_len) for s in sentences])\n",
        "\n",
        "\n",
        "train_en_idx = sentences_to_inds(train_en_tokenized, vocab_en, MAX_LEN)\n",
        "train_de_idx = sentences_to_inds(train_de_tokenized, vocab_de, MAX_LEN)\n",
        "\n",
        "dev_en_idx = sentences_to_inds(dev_en_tokenized, vocab_en, MAX_LEN)\n",
        "dev_de_idx = sentences_to_inds(dev_de_tokenized, vocab_de, MAX_LEN)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJfVdQX_tmfb",
        "colab_type": "text"
      },
      "source": [
        "Save to file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAuSUbq4reI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path = F\"/content/gdrive/My Drive/690D/train_en_idx.pkl\" \n",
        "# with open(path, \"wb\") as f:\n",
        "#     pickle.dump(train_en_idx, f)\n",
        "\n",
        "# path = F\"/content/gdrive/My Drive/690D/train_de_idx.pkl\" \n",
        "# with open(path, \"wb\") as f:\n",
        "#     pickle.dump(train_de_idx, f)\n",
        "\n",
        "# path = F\"/content/gdrive/My Drive/690D/dev_en_idx.pkl\" \n",
        "# with open(path, \"wb\") as f:\n",
        "#     pickle.dump(dev_en_idx, f)\n",
        "\n",
        "# path = F\"/content/gdrive/My Drive/690D/dev_de_idx.pkl\" \n",
        "# with open(path, \"wb\") as f:\n",
        "#     pickle.dump(dev_de_idx, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjeMEG5QtoOM",
        "colab_type": "text"
      },
      "source": [
        "Load from file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4h7gx5kr2Tx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path = F\"/content/gdrive/My Drive/690D/train_en_idx.pkl\" \n",
        "# with open(path, \"rb\") as f:\n",
        "#     train_en_idx = pickle.load(f)\n",
        "\n",
        "# path = F\"/content/gdrive/My Drive/690D/train_de_idx.pkl\" \n",
        "# with open(path, \"rb\") as f:\n",
        "#     train_de_idx = pickle.load(f)\n",
        "\n",
        "# path = F\"/content/gdrive/My Drive/690D/dev_en_idx.pkl\" \n",
        "# with open(path, \"rb\") as f:\n",
        "#     dev_en_idx = pickle.load(f)\n",
        "\n",
        "# path = F\"/content/gdrive/My Drive/690D/dev_de_idx.pkl\" \n",
        "# with open(path, \"rb\") as f:\n",
        "#     dev_de_idx = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roD4mC7Nt1K6",
        "colab_type": "text"
      },
      "source": [
        "Calculate sequence lengths. Will be used for making bins later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6X3Y6azANOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_en_lens = torch.LongTensor([n for n in (train_en_idx != PAD_IDX).sum(dim=1)])\n",
        "train_de_lens = torch.LongTensor([n for n in (train_de_idx != PAD_IDX).sum(dim=1)])\n",
        "\n",
        "dev_de_lens = torch.LongTensor([n for n in (dev_de_idx != PAD_IDX).sum(dim=1)])\n",
        "dev_en_lens = torch.LongTensor([n for n in (dev_en_idx != PAD_IDX).sum(dim=1)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B__N1MR9a7b3",
        "colab_type": "text"
      },
      "source": [
        "# GloVE embeddings\n",
        "\n",
        "Data loading adapted from: https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w81B91Kk6ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install bcolz\n",
        "\n",
        "glove_dim = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8Do3q6CJirG",
        "colab_type": "text"
      },
      "source": [
        "## Load EN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyL-9JfpawuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bcolz\n",
        "\n",
        "# https://drive.google.com/open?id=1YS8b5YGz462bTQ2PbJ0zjUkc29d2iwJb\n",
        "en_emb_vecs = drive.CreateFile({'id': '1YS8b5YGz462bTQ2PbJ0zjUkc29d2iwJb'})\n",
        "\n",
        "\n",
        "words = []\n",
        "idx = 0\n",
        "word2idx = {}\n",
        "glove_path = '.'\n",
        "cparams = bcolz.cparams(clevel=6)\n",
        "\n",
        "en_emb_vecs.GetContentFile(f'{glove_path}/glove.6B.{glove_dim}d.txt') \n",
        "vectors = bcolz.carray(np.zeros(1), rootdir=f'{glove_path}/6B.{glove_dim}.dat', mode='w', cparams=cparams)\n",
        "\n",
        "with open(f'{glove_path}/glove.6B.{glove_dim}d.txt', 'rb') as f:\n",
        "    for l in f:\n",
        "        line = l.decode().split()\n",
        "        word = line[0]\n",
        "        words.append(word)\n",
        "        word2idx[word] = idx\n",
        "        idx += 1\n",
        "        vect = np.array(line[1:]).astype(np.float)\n",
        "        vectors.append(vect)\n",
        "\n",
        "vectors = bcolz.carray(vectors[1:].reshape((-1, glove_dim)), rootdir=f'{glove_path}/6B.{glove_dim}.dat', mode='w', cparams=cparams)\n",
        "vectors.flush()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I5czb4cawsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove = {w: vectors[word2idx[w]] for w in words}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm9Xr8eAJlpW",
        "colab_type": "text"
      },
      "source": [
        "## Load DE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWpR5TND8AK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bcolz\n",
        "\n",
        "# https://drive.google.com/open?id=17zgrEtVQ-wbks4mqGDyVAqut7sUAXVYa\n",
        "\n",
        "de_emb_vecs = drive.CreateFile({'id': '17zgrEtVQ-wbks4mqGDyVAqut7sUAXVYa'})\n",
        "de_emb_vecs.GetContentFile('./de_embs.txt') \n",
        "\n",
        "words_de = []\n",
        "idx = 0\n",
        "word2idx_de = {}\n",
        "glove_path_de = '.'\n",
        "glove_dim = 300\n",
        "dat_file = f'{glove_path_de}/de.{glove_dim}.dat'\n",
        "cparams = bcolz.cparams(clevel=9, cname=\"zlib\")\n",
        "\n",
        "!rm -rf \"de.300.dat\"\n",
        "\n",
        "vectors_de = bcolz.carray(np.zeros(1), rootdir=dat_file, mode='w', cparams=cparams)\n",
        "\n",
        "with open('./de_embs.txt', 'rb') as f:\n",
        "    for l in f:\n",
        "        line = l.decode().split()\n",
        "        word = line[0]\n",
        "        words_de.append(word)\n",
        "        word2idx_de[word] = idx\n",
        "        idx += 1\n",
        "        vect = np.array(line[1:]).astype(np.float)\n",
        "        vectors_de.append(vect)\n",
        "        \n",
        "vectors_de = bcolz.carray(vectors_de[1:].reshape((-1, glove_dim)), rootdir=dat_file, mode='w', cparams=cparams)\n",
        "vectors_de.flush()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glZlH3lZ6MJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_de = {w: vectors_de[word2idx_de[w]] for w in words_de}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iVJpOsxuatN",
        "colab_type": "text"
      },
      "source": [
        "Save to file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_igRhW9Ci06l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path = F\"/content/gdrive/My Drive/690D/glove.pkl\" \n",
        "# with open(path, \"wb\") as f:\n",
        "#     pickle.dump(glove, f)\n",
        "\n",
        "# path = F\"/content/gdrive/My Drive/690D/glove_de.pkl\" \n",
        "# with open(path, \"wb\") as f:\n",
        "#     pickle.dump(glove_de, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwaQaRY3ucU-",
        "colab_type": "text"
      },
      "source": [
        "Load from file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e38VxoN74atf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path = F\"/content/gdrive/My Drive/690D/glove.pkl\" \n",
        "# with open(path, \"rb\") as f:\n",
        "#     glove = pickle.load(f)\n",
        "\n",
        "# path = F\"/content/gdrive/My Drive/690D/glove_de.pkl\" \n",
        "# with open(path, \"rb\") as f:\n",
        "#     glove_de = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhxnGp9nKELT",
        "colab_type": "text"
      },
      "source": [
        "## Prep embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3oR44Wfc8OY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def create_glove_embeddings(target_vocab, word2vec, emb_dim):\n",
        "    matrix_len = len(target_vocab)\n",
        "    weights_matrix = np.zeros((matrix_len, emb_dim))\n",
        "    words_found = 0\n",
        "\n",
        "    for word, idx in target_vocab.items():\n",
        "        try: \n",
        "            weights_matrix[idx] = word2vec[word]\n",
        "            words_found += 1\n",
        "        except KeyError:\n",
        "            weights_matrix[idx] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
        "    print(f'{words_found} words found')\n",
        "    return weights_matrix\n",
        "\n",
        "glove_embeddings = create_glove_embeddings(vocab_en, glove, glove_dim)\n",
        "glove_embeddings_de = create_glove_embeddings(vocab_de, glove_de, glove_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMqQJZaCulfi",
        "colab_type": "text"
      },
      "source": [
        "Save:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whiF-Zc_l3Um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path = F\"/content/gdrive/My Drive/690D/glove_embeddings.pkl\" \n",
        "# with open(path, \"wb\") as f:\n",
        "#     pickle.dump(glove_embeddings, f)\n",
        "\n",
        "# path = F\"/content/gdrive/My Drive/690D/glove_embeddings_de.pkl\" \n",
        "# with open(path, \"wb\") as f:\n",
        "#     pickle.dump(glove_embeddings_de, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IhsAYfbumqZ",
        "colab_type": "text"
      },
      "source": [
        "Load:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zliiCmQK2cb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path = F\"/content/gdrive/My Drive/690D/glove_embeddings.pkl\" \n",
        "# with open(path, \"rb\") as f:\n",
        "#     glove_embeddings = pickle.load(f)\n",
        "\n",
        "# path = F\"/content/gdrive/My Drive/690D/glove_embeddings_de.pkl\" \n",
        "# with open(path, \"rb\") as f:\n",
        "#     glove_embeddings_de = pickle.load(f)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk_POrHOynjx",
        "colab_type": "text"
      },
      "source": [
        "# Character Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDTvjuiGvMST",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JqbrUp4lG4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "de_chars = [torch.FloatTensor([ord(c) for c in w]) for w in vocab_de.keys()]\n",
        "en_chars = [torch.FloatTensor([ord(c) for c in w]) for w in vocab_en.keys()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKeJD6DGnWiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(max(len(cs) for cs in de_chars))\n",
        "print(sum(len(cs) for cs in de_chars) / len(de_chars))\n",
        "\n",
        "print(max(len(cs) for cs in en_chars))\n",
        "print(sum(len(cs) for cs in en_chars) / len(de_chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po_wlCU8n6FL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "de_chars.append(torch.zeros((100,)))\n",
        "de_chars_train = nn.utils.rnn.pad_sequence(de_chars, batch_first=True, padding_value=-1)\n",
        "de_chars_train = de_chars_train[:-1, :]\n",
        "print(de_chars_train = de_chars_train[:-1, :])\n",
        "\n",
        "en_chars.append(torch.zeros((100,)))\n",
        "en_chars_train = nn.utils.rnn.pad_sequence(en_chars, batch_first=True, padding_value=-1)\n",
        "en_chars_train = en_chars_train[:-1, :]\n",
        "print(en_chars_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njy5JBaHvGhM",
        "colab_type": "text"
      },
      "source": [
        "## The AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-jOVJQcymeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class CharCoder(nn.Module):\n",
        "    def __init__(self, d_in):\n",
        "        super(CharCoder, self).__init__()\n",
        "        \n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Linear(d_in, 1024),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Linear(256, 64)\n",
        "        )\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.Linear(64, 256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Linear(256, 1024),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Linear(1024, d_in)\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        out = self.dec(self.enc(X))\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuSlavPHvJ61",
        "colab_type": "text"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVTcG6C03cgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_cc(X, model, params, optimizer, print_every=10):\n",
        "\n",
        "    criterion = nn.MSELoss().cuda()\n",
        "    \n",
        "    N = X.shape[0]\n",
        "    \n",
        "    b_sz = params['batch_size']\n",
        "    batches = [(start, start + b_sz) for start in range(0, N, b_sz)]\n",
        "    \n",
        "    model.train()\n",
        "    for epoch in range(params['epochs']):\n",
        "        ep_loss = 0.\n",
        "        start_time = time.time()\n",
        "        random.shuffle(batches)\n",
        "\n",
        "        # for each batch, calculate loss and optimize model parameters\n",
        "        for b_idx, (start, end) in enumerate(batches):\n",
        "            x = X[start:end].cuda()\n",
        "            mask = x >= 0\n",
        "            \n",
        "            preds = model(x)\n",
        "\n",
        "            loss = criterion(preds[mask], x[mask])\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            ep_loss += loss.detach().cpu()\n",
        "        \n",
        "        if (epoch + 1) % print_every == 0:\n",
        "            print('epoch: %d, loss: %0.4f, time: %0.2f sec' % \\\n",
        "                  (epoch, ep_loss, time.time() - start_time))\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z73vyiadvSyE",
        "colab_type": "text"
      },
      "source": [
        "## Train DE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXxUYkfp0kdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "cc_de = CharCoder(100).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsTbRzESwxxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cparams = {}\n",
        "cparams['batch_size'] = 8192*4\n",
        "cparams['epochs'] = 200\n",
        "\n",
        "coptimizer_de = optim.Adam(cc_de.parameters(), lr=0.000001)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "train_cc(de_chars_train, cc_de, cparams, coptimizer_de)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR-h50o3vV6-",
        "colab_type": "text"
      },
      "source": [
        "## Train EN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItsNX4XfvpNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "cc_en = CharCoder(100).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2KvixWEvr5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cparams = {}\n",
        "cparams['batch_size'] = 8192*4\n",
        "cparams['epochs'] = 200\n",
        "\n",
        "coptimizer_en = optim.Adam(cc_en.parameters(), lr=0.00001)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "train_cc(en_chars_train, cc_en, cparams, coptimizer_en)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QraP5vBovaoL",
        "colab_type": "text"
      },
      "source": [
        "Save:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za-YKtuizZJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# import pickle\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# model_save_name = 'cc_en.pt'\n",
        "# path = F\"/content/gdrive/My Drive/690D/{model_save_name}\" \n",
        "# torch.save(cc_en.state_dict(), path)\n",
        "\n",
        "\n",
        "# model_save_name = 'cc_de.pt'\n",
        "# path = F\"/content/gdrive/My Drive/690D/{model_save_name}\" \n",
        "# torch.save(cc_de.state_dict(), path)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_2xz-7o2M2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_save_name = 'cc_en.pt'\n",
        "# path = F\"/content/gdrive/My Drive/690D/{model_save_name}\" \n",
        "# cc_en.load_state_dict(torch.load(path))\n",
        "\n",
        "# model_save_name = 'cc_de.pt'\n",
        "# path = F\"/content/gdrive/My Drive/690D/{model_save_name}\" \n",
        "# cc_de.load_state_dict(torch.load(path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjejXNr6vndR",
        "colab_type": "text"
      },
      "source": [
        "Create embeddings for all token in vocab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWRylLEU2toD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_char_embeddings(target_vocab, chars, cc_model):\n",
        "    \n",
        "    char_embs_ = cc_model.enc(chars.cuda()).detach().cpu()\n",
        "    emb_dim = char_embs_.shape[1]\n",
        "    \n",
        "    matrix_len = len(target_vocab)\n",
        "    weights_matrix = np.zeros((matrix_len, emb_dim))\n",
        "\n",
        "    for word, idx in target_vocab.items():\n",
        "        weights_matrix[idx] = char_embs_[idx]\n",
        "\n",
        "    return weights_matrix\n",
        "\n",
        "char_embs_en = create_char_embeddings(vocab_en, en_chars_train, cc_en)\n",
        "char_embs_de = create_char_embeddings(vocab_de, de_chars_train, cc_de)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI4ljUDuvb8j",
        "colab_type": "text"
      },
      "source": [
        "Load:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQFMS1Q16X47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# import pickle\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# path = F\"/content/gdrive/My Drive/690D/char_embs_en.pkl\" \n",
        "# with open(path, \"wb\") as f:\n",
        "#     pickle.dump(char_embs_en, f)\n",
        "\n",
        "# path = F\"/content/gdrive/My Drive/690D/char_embs_de.pkl\" \n",
        "# with open(path, \"wb\") as f:\n",
        "#     pickle.dump(char_embs_de, f)\n",
        "\n",
        "path = F\"/content/gdrive/My Drive/690D/char_embs_en.pkl\"\n",
        "with open(path, \"rb\") as f:\n",
        "    char_embs_en = pickle.load(f)\n",
        "\n",
        "path = F\"/content/gdrive/My Drive/690D/char_embs_de.pkl\"\n",
        "with open(path, \"rb\") as f:\n",
        "    char_embs_de = pickle.load(f)\n",
        "\n",
        "print(char_embs_en.shape)\n",
        "print(char_embs_de.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5aP2duuvsrz",
        "colab_type": "text"
      },
      "source": [
        "## Concatenate with GloVE embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCXL0Jnf8jEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_embeddings = np.concatenate((char_embs_en, glove_embeddings), axis=1)\n",
        "glove_embeddings_de = np.concatenate((char_embs_de, glove_embeddings_de), axis=1)\n",
        "\n",
        "print(glove_embeddings.shape)\n",
        "print(glove_embeddings_de.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tqALjMev8EZ",
        "colab_type": "text"
      },
      "source": [
        "---------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJse7UtgANOW",
        "colab_type": "text"
      },
      "source": [
        "# Transformer\n",
        "\n",
        "A cleaner, though probably less efficient, implementation than others I have seen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqHqzb0NANOX",
        "colab_type": "text"
      },
      "source": [
        "#### Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGWEw65LFMQ3",
        "colab_type": "code",
        "outputId": "a13275e2-906c-4294-f6dc-09bbb6147ddc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "@lru_cache(maxsize=16)\n",
        "def get_mask(seq_len):\n",
        "    return (torch.ones((seq_len, seq_len)).tril() == 1).cuda() # valid positions\n",
        "\n",
        "\n",
        "plt.imshow(get_mask(20).cpu())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc03827e358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_twyM8uANOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "   \n",
        "class AttentionCell(nn.Module):\n",
        "    def __init__(self, d_in, d_k, d_z):\n",
        "        super(AttentionCell, self).__init__()\n",
        "        \n",
        "        self.Wq = nn.Linear(d_in, d_k)\n",
        "        self.Wk = nn.Linear(d_in, d_k)\n",
        "        self.Wv = nn.Linear(d_in, d_z)\n",
        "        self.scaling = d_k**(-0.5)\n",
        "        \n",
        "    def forward(self, Q, K, V, mask):\n",
        "        \n",
        "        q = self.Wq(Q)\n",
        "        k = self.Wk(K)\n",
        "        v = self.Wv(V)\n",
        "        \n",
        "        \n",
        "        # (N, seq_len, d_k) x (N, d_k, seq_len) -> (N, seq_len, seq_len)\n",
        "        scores = torch.bmm(q, k.transpose(1, 2)) * self.scaling\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e10)\n",
        "        scores = F.softmax(scores, dim=-1)\n",
        "        \n",
        "        z = torch.bmm(scores, v)\n",
        "\n",
        "        return z\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ccajr_uLANOa",
        "colab_type": "text"
      },
      "source": [
        "#### Multi-head attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIzu46K0ANOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttentionCell(nn.Module):\n",
        "    def __init__(self, nheads, d_in, d_k, d_z, p_dropout=0.1):\n",
        "        super(MultiHeadAttentionCell, self).__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList(\n",
        "            [AttentionCell(d_in, d_k, d_z) for _ in range(nheads)]\n",
        "        )\n",
        "        self.Wo = nn.Sequential(\n",
        "            nn.Linear(d_z * nheads, d_in),\n",
        "            nn.Dropout(p_dropout)\n",
        "        )\n",
        "        \n",
        "        \n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \n",
        "        zs = torch.cat([head(Q, K, V, mask) for head in self.heads], dim=2)\n",
        "        z = self.Wo(zs)\n",
        "        \n",
        "        return z\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_AtXNkwANOc",
        "colab_type": "text"
      },
      "source": [
        "#### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVB3I-qaANOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderCell(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(EncoderCell, self).__init__()\n",
        "        \n",
        "        d_in, d_k, d_z = params['d_in'], params['d_k'], params['d_z']\n",
        "        d_out = params['d_out']\n",
        "        nheads = params['nheads']\n",
        "        dropout_attn = params['dropout_attn']\n",
        "        d_h = params['d_h']\n",
        "        \n",
        "        self.SelfAttentionBlock = MultiHeadAttentionCell(nheads, d_in, d_k, d_z, dropout_attn)\n",
        "        self.norm1 = nn.LayerNorm(d_in)\n",
        "        \n",
        "        self.We = nn.Sequential(\n",
        "            nn.Linear(d_in, d_h),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(params['dropout_ff']),\n",
        "            nn.Linear(d_h, d_out)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(d_in)\n",
        "        \n",
        "        \n",
        "    def forward(self, inp):\n",
        "        X, src_mask = inp\n",
        "        z = self.SelfAttentionBlock(X, X, X, src_mask)\n",
        "        norm1 = self.norm1(X + z)\n",
        "        e = self.We(norm1)\n",
        "        norm2 = self.norm2(z + e)\n",
        "        \n",
        "        return norm2, src_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVSzAiYKANOf",
        "colab_type": "text"
      },
      "source": [
        "#### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTHvRhPIANOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class DecoderCell(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(DecoderCell, self).__init__()\n",
        "        \n",
        "        d_in, d_k, d_z = params['d_in'], params['d_k'], params['d_z']\n",
        "        d_out = params['d_out']\n",
        "        nheads_attn, nheads_self_attn = params['nheads_attn'], params['nheads_self_attn']\n",
        "        dropout_attn, dropout_self_attn = params['dropout_attn'], params['dropout_self_attn']\n",
        "        d_h = params['d_h']\n",
        "        \n",
        "        self.SelfAttentionBlock = MultiHeadAttentionCell(nheads_self_attn, d_in, d_k, d_z, dropout_self_attn)\n",
        "        self.EncDecAttentionBlock = MultiHeadAttentionCell(nheads_attn, d_in, d_k, d_z, dropout_attn)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_in)\n",
        "        self.norm2 = nn.LayerNorm(d_in)\n",
        "        self.norm3 = nn.LayerNorm(d_in)\n",
        "        \n",
        "        self.Wd = nn.Sequential(\n",
        "            nn.Linear(d_in, d_h),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(params['dropout_ff']),\n",
        "            nn.Linear(d_h, d_out)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \n",
        "        X, enc_out, src_mask, tgt_mask = inp\n",
        "        self_attn_out = self.SelfAttentionBlock(X, X, X, mask=tgt_mask)\n",
        "        norm1 = self.norm1(X + self_attn_out)\n",
        "        \n",
        "        encdec_attn_out = self.EncDecAttentionBlock(norm1, enc_out, enc_out, mask=src_mask)\n",
        "        norm2 = self.norm2(norm1 + encdec_attn_out)\n",
        "        \n",
        "        ff_out = self.Wd(norm2)\n",
        "        norm3 = self.norm2(norm2 + ff_out)\n",
        "        out = norm3\n",
        "        \n",
        "        return (out, enc_out, src_mask, tgt_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgkQj9qvANOi",
        "colab_type": "text"
      },
      "source": [
        "#### Positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKkdaqJUANOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "@lru_cache(maxsize=128)\n",
        "def get_positional_embeddings(seq_len, d_emb):\n",
        "\n",
        "    js = (torch.arange(d_emb).float() // 2) * 2\n",
        "    denom = (10000**(js / d_emb)).reshape(1, -1)\n",
        "\n",
        "    i = torch.arange(seq_len).reshape(-1, 1).repeat((1, d_emb)).float()\n",
        "\n",
        "    pos_embed = i / denom\n",
        "\n",
        "    is_even_ind = np.arange(d_emb) % 2 == 0\n",
        "    even_inds = np.where(is_even_ind)\n",
        "    odd_inds = np.where(1 - is_even_ind)\n",
        "\n",
        "    pos_embed[:, even_inds] = np.sin(pos_embed[:, even_inds])\n",
        "    pos_embed[:, odd_inds] = np.cos(pos_embed[:, odd_inds])\n",
        "    \n",
        "    return pos_embed.cuda()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocEXMZ29ANOk",
        "colab_type": "text"
      },
      "source": [
        "#### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSuCeSXYANOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, params, embeddings_in=None, embeddings_out=None):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        in_vocab_size, out_vocab_size = params['in_vocab_size'], params['out_vocab_size']\n",
        "        d_emb_in, d_emb_out = params['d_emb_in'], params['d_emb_out']\n",
        "        self.embeddings_in = nn.Embedding(in_vocab_size, d_emb_in)\n",
        "        self.embeddings_out = nn.Embedding(out_vocab_size, d_emb_out)\n",
        "        \n",
        "        if embeddings_in is not None:\n",
        "            self.embeddings_in.weight.data = torch.from_numpy(embeddings_in).float().cuda()\n",
        "            self.embeddings_in.weight.requires_grad = False\n",
        "        if embeddings_out is not None:\n",
        "            self.embeddings_out.weight.data = torch.from_numpy(embeddings_out).float().cuda()\n",
        "            self.embeddings_out.weight.requires_grad = False\n",
        "        \n",
        "        nencoders = params['nencoders']\n",
        "        ndecoders = params['ndecoders']\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "            *[EncoderCell(params['encoder']) for _ in range(nencoders)]\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            *[DecoderCell(params['decoder']) for _ in range(ndecoders)]\n",
        "        )\n",
        "        \n",
        "        self.Wt = nn.Linear(params['decoder']['d_out'], out_vocab_size)\n",
        "        \n",
        "        \n",
        "    def forward(self, X, Y, src_mask=None, tgt_mask=None):\n",
        "        \n",
        "        encoding = self.encode_(X, src_mask=src_mask)\n",
        "        decoding = self.decode_(Y, encoding, src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "        logits = self.Wt(decoding)\n",
        "        \n",
        "        return logits\n",
        "    \n",
        "    def encode_(self, X, src_mask=None):\n",
        "        embs_in = self.embeddings_in(X)\n",
        "        embs_in += get_positional_embeddings(embs_in.shape[1], embs_in.shape[2])\n",
        "        encoding, _ = self.encoder((embs_in, src_mask))\n",
        "        return encoding\n",
        "    \n",
        "    def decode_(self, Y, encoding, src_mask=None, tgt_mask=None):\n",
        "        embs_out = self.embeddings_out(Y)\n",
        "        embs_out += get_positional_embeddings(embs_out.shape[1], embs_out.shape[2])\n",
        "        decoding, _, _, _ = self.decoder((embs_out, encoding, src_mask, tgt_mask))\n",
        "        return decoding\n",
        "\n",
        "    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Nr_ab6ANOo",
        "colab_type": "text"
      },
      "source": [
        "# Make Model\n",
        "\n",
        "d_in should be set to 364 if using GloVE + Character embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpfhzUDNANOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = None\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "in_vocab  = vocab_de\n",
        "out_vocab = vocab_en\n",
        "\n",
        "# model\n",
        "# seq_len = train_X.shape[1]\n",
        "d_in = 300\n",
        "# d_in = 364\n",
        "d_k = 64\n",
        "d_z = d_k * 2\n",
        "d_h = d_in * 1\n",
        "\n",
        "## model\n",
        "model_params = {}\n",
        "model_params['in_vocab_size'] = len(in_vocab)\n",
        "model_params['out_vocab_size'] = len(out_vocab)\n",
        "model_params['d_emb_in'] = d_in\n",
        "model_params['d_emb_out'] = d_in\n",
        "model_params['nencoders'] = 2\n",
        "model_params['ndecoders'] = 2\n",
        "# encoder params\n",
        "model_params['encoder'] = {}\n",
        "model_params['encoder']['nheads'] = 4\n",
        "model_params['encoder']['d_in'] = d_in\n",
        "model_params['encoder']['d_k']  = d_k\n",
        "model_params['encoder']['d_z']  = d_z\n",
        "model_params['encoder']['d_h']  = d_h\n",
        "model_params['encoder']['d_out'] = d_in\n",
        "model_params['encoder']['dropout_ff'] = 0.1\n",
        "model_params['encoder']['dropout_attn']  = 0.1\n",
        "# decoder params\n",
        "model_params['decoder'] = {}\n",
        "model_params['decoder']['nheads_attn'] = 4\n",
        "model_params['decoder']['nheads_self_attn'] = 4\n",
        "model_params['decoder']['d_in'] = d_in\n",
        "model_params['decoder']['d_k']  = d_k\n",
        "model_params['decoder']['d_z']  = d_z\n",
        "model_params['decoder']['d_h']  = d_h\n",
        "model_params['decoder']['d_out'] = d_in\n",
        "model_params['decoder']['dropout_ff'] = 0.1\n",
        "model_params['decoder']['dropout_attn']  = 0.1\n",
        "model_params['decoder']['dropout_self_attn']  = 0.1\n",
        "\n",
        "model = None\n",
        "# model = Transformer(model_params).cuda()\n",
        "model = Transformer(model_params, embeddings_in=glove_embeddings_de, embeddings_out=glove_embeddings).cuda()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEJJjYJUH1f0",
        "colab_type": "code",
        "outputId": "70eba317-cb47-49b3-f855-84bd2c325ab9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# nick = '4,4,4,4,r'\n",
        "# model_save_name = f'transformer{nick}.pt'\n",
        "# path = F\"/content/gdrive/My Drive/690D/{model_save_name}\" \n",
        "# model.load_state_dict(torch.load(path))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4QpZM50ANOr",
        "colab_type": "text"
      },
      "source": [
        "# Translate\n",
        "\n",
        "Define Greedy decoding and Beam Search decoding functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXcJnjxrYW2s",
        "colab_type": "text"
      },
      "source": [
        "## Greedy decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBvPtz7PYF1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_helper_greedy(model, out, encoding, src_mask, tgt_mask):\n",
        "    \n",
        "    out[:, 0] = BOS_IDX\n",
        "\n",
        "    for i in range(1, out.shape[1]):\n",
        "        decoding = model.decode_(out, encoding, src_mask, tgt_mask).detach()\n",
        "        logits = model.Wt(decoding[:, i]).detach()\n",
        "        scores = F.softmax(logits, dim=-1).detach()\n",
        "        sampled_word = torch.argmax(scores, dim=-1)\n",
        "        out[:, i] = sampled_word\n",
        "\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-ziAtEIYaCd",
        "colab_type": "text"
      },
      "source": [
        "## Beam search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m9Z7DenYI59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_helper_beam_search(model, out, encoding, src_mask, tgt_mask, k=4, debug=False):\n",
        "    \n",
        "    beam_p = torch.ones((out.shape[0], k))\n",
        "    beam_idx = torch.zeros((out.shape[0], out.shape[1], k)) + BOS_IDX\n",
        "    \n",
        "    tmp_p = torch.zeros((out.shape[0], k*k))\n",
        "    tmp_idx = torch.zeros((out.shape[0], k*k))\n",
        "    \n",
        "    not_ended = [True] * k\n",
        "    for i in range(1, out.shape[1] - 1):\n",
        "        if sum(not_ended) == 0:\n",
        "            break\n",
        "        for j in range(k):\n",
        "            \n",
        "            out[:, : i] = beam_idx[:, : i, j]\n",
        "            ended = (out[:, : i] == EOS_IDX).sum(dim=1) > 0\n",
        "            \n",
        "            if (1 - ended).sum() == 0:\n",
        "                not_ended[j] = False\n",
        "                continue\n",
        "            \n",
        "            decoding = model.decode_(out, encoding, src_mask, tgt_mask).detach()\n",
        "            logits = model.Wt(decoding[:, i - 1]).detach()\n",
        "            scores = F.softmax(logits, dim=-1).detach()\n",
        "            \n",
        "            p, idx = torch.topk(scores.cpu(), k, largest=True, dim=-1)\n",
        "            p[ended, -1] = 1\n",
        "            p[ended, :-1] = 0\n",
        "            idx[ended, -1] = PAD_IDX\n",
        "            tmp_p[:, j : j + k] = beam_p[:, j].unsqueeze(-1) * p\n",
        "            tmp_idx[:, j : j + k] = idx\n",
        "            \n",
        "            if i == 1:\n",
        "                break\n",
        "        tmp_p, sort_idx = torch.sort(tmp_p, dim=1, descending=True)\n",
        "        tmp_idx = torch.gather(tmp_idx, 1, sort_idx)\n",
        "        beam_p = tmp_p[:, : k]\n",
        "        beam_idx[:, i, :] = tmp_idx[:, : k]\n",
        "        \n",
        "        if debug:\n",
        "            for m in range(k):\n",
        "                for r, row in enumerate(beam_idx[:, : i + 1, m]):\n",
        "                    row = takewhile(lambda i: i != EOS_IDX, row)\n",
        "                    print(beam_p[r, m], ' '.join(idx_to_word_en[int(w)] for w in row))\n",
        "            print()\n",
        "        \n",
        "    \n",
        "    return beam_idx[:, :, 0].cpu().long()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDnuRxBHANOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import takewhile\n",
        "\n",
        "\n",
        "def inds_to_sentences(sentence_inds, idx_to_word):\n",
        "    truncated = [takewhile(lambda i: i != EOS_IDX, s[1:]) for s in sentence_inds]\n",
        "    return [' '. join(idx_to_word[idx] for idx in s) for s in truncated]\n",
        "\n",
        "\n",
        "def decode(model, X, max_len, greedy=False):\n",
        "    src_mask = (X != PAD_IDX).unsqueeze(-2)\n",
        "    tgt_mask = get_mask(max_len)\n",
        "    encoding = model.encode_(X, src_mask)\n",
        "    out = torch.zeros((X.shape[0], max_len)).long().cuda() + PAD_IDX\n",
        "    if greedy:\n",
        "        decoding = decode_helper_greedy(model, out, encoding, src_mask, tgt_mask)\n",
        "    else:\n",
        "        decoding = decode_helper_beam_search(model, out, encoding, src_mask, tgt_mask)\n",
        "    return decoding\n",
        "\n",
        "\n",
        "def translate(model, in_sentences, in_tokenizer, in_vocab, out_idx_to_word, max_len, greedy=False):\n",
        "    \n",
        "    tokenized = [in_tokenizer(s.strip().lower()) for s in in_sentences]\n",
        "    tokenized = [[BOS_] + [token.text for token in sentence] + [EOS_] for sentence in tokenized]\n",
        "    \n",
        "    X = sentences_to_inds(tokenized, in_vocab, max_len).cuda()\n",
        "    model.eval()\n",
        "    model_out = decode(model, X, max_len)\n",
        "    translation = inds_to_sentences(model_out, out_idx_to_word)\n",
        "    return translation\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mseb2gjTYpKN",
        "colab_type": "text"
      },
      "source": [
        "## Test translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXDJKE06Yoma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = []\n",
        "sentences.append('das ist sehr gut')\n",
        "sentences.append('brot und wasser')\n",
        "sentences.append('ich bin einen apfel')\n",
        "sentences.append('Wir müssen wissen - wir werden wissen!')\n",
        "print(sentences)\n",
        "\n",
        "# translate\n",
        "model.eval()\n",
        "translation = translate(model, sentences, nlp_de.tokenizer, vocab_de, idx_to_word_en, max_len=MAX_LEN, greedy=False)\n",
        "for s in translation:\n",
        "    print(s)\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4YnvkchANOs",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxdxL6FBxbub",
        "colab_type": "text"
      },
      "source": [
        "## The training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFbEqq_LANOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_transformer(train_seq_bins, dev_seq_bins, model, params, optimizer, max_epoch_iters=100):\n",
        "\n",
        "    target_vocab_sz = params['out_vocab_size']\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX).cuda()\n",
        "    \n",
        "    model.train()\n",
        "    for epoch in range(params['epochs']):\n",
        "        ep_loss1 = 0.\n",
        "        ep_loss2 = 0.\n",
        "        ep_start_time = time.time()\n",
        "\n",
        "        for i, (X, Y, bsz) in enumerate(train_seq_bins):\n",
        "            N = X.shape[0]\n",
        "            batches = [(start, start + bsz) for start in range(0, N, bsz)]\n",
        "            random.shuffle(batches)\n",
        "            b_loss1 = 0.\n",
        "            b_loss2 = 0.\n",
        "            b_start_time = time.time()\n",
        "            \n",
        "            for b_idx, (start, end) in enumerate(batches):\n",
        "\n",
        "                x = X[start:end].contiguous().cuda()\n",
        "                y = Y[start:end].contiguous().cuda()\n",
        "                \n",
        "                src_mask = (x != PAD_IDX).unsqueeze(-2)\n",
        "                y_nonpad = (y != PAD_IDX)\n",
        "                tgt_mask = y_nonpad.unsqueeze(-2) & get_mask(y.shape[1])\n",
        "                \n",
        "                preds = model(x, y, src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "                preds = preds[:, :-1, :].contiguous().view(-1, target_vocab_sz)\n",
        "                labels = y[:, 1:].contiguous().view(-1)\n",
        "                \n",
        "                loss1 = criterion(preds, labels)\n",
        "\n",
        "                loss = loss1\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                b_loss1 += loss1.detach().cpu()\n",
        "        \n",
        "            print('bin: %d, loss: %0.4f, loss_emb: %0.4f, time: %0.2f sec' % \\\n",
        "                  (i, b_loss1, b_loss2, time.time() - b_start_time))\n",
        "        \n",
        "            ep_loss1 += b_loss1.detach().cpu()\n",
        "#             ep_loss2 += b_loss2.detach().cpu()\n",
        "        print('>> epoch: %d, loss: %0.4f, loss_emb: %0.4f, time: %0.2f sec' % \\\n",
        "              (epoch, ep_loss1, ep_loss2, time.time() - ep_start_time))\n",
        "        \n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            model.eval()\n",
        "            dev_loss = 0.\n",
        "            for i, (X, Y, bsz) in enumerate(dev_seq_bins):\n",
        "                N = X.shape[0]\n",
        "                batches = [(start, start + bsz) for start in range(0, N, bsz)]\n",
        "                dev_start_time = time.time()\n",
        "\n",
        "                for b_idx, (start, end) in enumerate(batches):\n",
        "\n",
        "                    x = X[start:end].contiguous().cuda()\n",
        "                    y = Y[start:end].contiguous().cuda()\n",
        "\n",
        "                    src_mask = (x != PAD_IDX).unsqueeze(-2)\n",
        "                    tgt_mask = (y != PAD_IDX).unsqueeze(-2) & get_mask(y.shape[1])\n",
        "\n",
        "                    preds = model(x, y, src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "                    preds = preds[:, :-1, :].contiguous().view(-1, target_vocab_sz)\n",
        "                    labels = y[:, 1:].contiguous().view(-1)\n",
        "\n",
        "                    dev_loss += criterion(preds, labels).detach().cpu()\n",
        "            print('>>>> val: loss: %0.4f, loss_emb: %0.4f, time: %0.2f sec' % \\\n",
        "              (dev_loss, 0, time.time() - dev_start_time))\n",
        "            \n",
        "            print()\n",
        "            translation = translate(model, sentences, nlp_de.tokenizer, vocab_de, idx_to_word_en, MAX_LEN)\n",
        "            for s in translation:\n",
        "                print(s)\n",
        "            print()\n",
        "            model.train()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jzrco2Ixn2S",
        "colab_type": "text"
      },
      "source": [
        "## Make bins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBydvsikS47L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len_bins = [8, 12, 16, 20, 24, 32, 40, 64, 100]\n",
        "\n",
        "last = 0\n",
        "for i in len_bins:\n",
        "    n = ((last < train_de_lens) & (train_de_lens <= i)).sum()\n",
        "#     print(i, n, n - last)\n",
        "    last = i\n",
        "    \n",
        "last = 0\n",
        "for i in len_bins:\n",
        "    n = (dev_de_lens <= i).sum()\n",
        "#     print(i, n, n - last)\n",
        "    last = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "480oIHyd22qK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bin_batch_sizes = (np.array([768, 512, 400, 320, 256, 200, 160, 100, 50]) / 1.).astype(int)\n",
        "# bin_batch_sizes = np.array([700, 400, 320, 256, 200, 160, 100, 64, 32]) // 2\n",
        "\n",
        "assert len(bin_batch_sizes) == len(len_bins)\n",
        "\n",
        "\n",
        "def make_seq_bins(X, X_lens, Y, Y_lens, len_bins, bin_batch_sizes, margin=4):\n",
        "    \n",
        "    seq_bins = []\n",
        "    \n",
        "    last_i = 0\n",
        "    for i, bsz in zip(len_bins, bin_batch_sizes):\n",
        "        mask = ((last_i < X_lens) & (X_lens <= i))\n",
        "        xs, ys = X[mask], Y[mask]\n",
        "        y_lens = Y_lens[mask]\n",
        "\n",
        "        is_long = y_lens > i + margin\n",
        "        is_short =  1 - is_long\n",
        "\n",
        "        short_xs, short_ys = xs[is_short, : i], ys[is_short, : i + margin]\n",
        "        max_y_len = y_lens.max()\n",
        "        long_xs, long_ys = xs[is_long, : i], ys[is_long, : max_y_len]\n",
        "\n",
        "        if is_long.sum() == 0:\n",
        "            break\n",
        "\n",
        "        long_bsz = bsz // 4\n",
        "        \n",
        "#         print(i, (is_short).sum(), bsz)\n",
        "#         print(i, (is_long).sum(), long_bsz)\n",
        "\n",
        "        seq_bins.append((short_xs, short_ys, bsz))\n",
        "        seq_bins.append((long_xs, long_ys, long_bsz))\n",
        "        \n",
        "        last_i = i\n",
        "    return seq_bins\n",
        "\n",
        "# print('Train')\n",
        "train_seq_bins = make_seq_bins(train_de_idx, train_de_lens, train_en_idx, train_en_lens, len_bins, bin_batch_sizes)\n",
        "# print('Dev')\n",
        "dev_seq_bins = make_seq_bins(dev_de_idx, dev_de_lens, dev_en_idx, dev_en_lens, len_bins, bin_batch_sizes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7ej-J-exwRS",
        "colab_type": "text"
      },
      "source": [
        "## Train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsrqkSQOANOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(sentences)\n",
        "## training\n",
        "train_params = {}\n",
        "train_params['out_vocab_size'] = model_params['out_vocab_size']\n",
        "train_params['learning_rate'] = 1e-4\n",
        "# train_params['batch_size'] = 128\n",
        "train_params['epochs'] = 4\n",
        "\n",
        "# optimizer = None\n",
        "optimizer = optim.Adam(model.parameters(), lr=train_params['learning_rate'], weight_decay=0)\n",
        "\n",
        "print(train_params)\n",
        "print()\n",
        "\n",
        "model.train()\n",
        "torch.cuda.empty_cache()\n",
        "train_transformer(train_seq_bins, dev_seq_bins, model, train_params, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYItY77Ewzwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-_-6rcsx5DN",
        "colab_type": "text"
      },
      "source": [
        "Save model to file. Super important."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7DbInTBmUHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# import pickle\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# nick = '4,4,4,4,cc2'\n",
        "\n",
        "# model_save_name = f'transformer{nick}.pt'\n",
        "# path = F\"/content/gdrive/My Drive/690D/{model_save_name}\" \n",
        "# torch.save(model.state_dict(), path)\n",
        "\n",
        "\n",
        "# model_params_save_name = f'params{nick}.pt'\n",
        "# path = F\"/content/gdrive/My Drive/690D/{model_params_save_name}\" \n",
        "# with open(path, \"wb\") as f:\n",
        "#     pickle.dump(model_params, f)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6_xpEoc9ag_",
        "colab_type": "text"
      },
      "source": [
        "# BLEU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuFuVO9FyK6u",
        "colab_type": "text"
      },
      "source": [
        "## Fetch data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80UQQmkN9dBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/moses-smt/mosesdecoder.git\n",
        "!pip install sacrebleu\n",
        "# # !pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "print('Authentication Successful!')\n",
        "iwslt_data = drive.CreateFile({'id': '1Swpb8yG3atDzxiOR0dQuFE-3KjgZCuWE'})\n",
        "iwslt_data.GetContentFile('iwslt_en_de.zip')\n",
        "\n",
        "!unzip iwslt_en_de.zip dev.en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTP7XhqLyHwI",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLKbFJozDrYH",
        "colab_type": "code",
        "outputId": "26ea9e9c-670e-4b3e-e5cd-7cf31cc95db4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "dev_de = open('dev.de', encoding=\"utf8\").read().strip().split('\\n')\n",
        "print(dev_de[:2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Als ich in meinen 20ern war, hatte ich meine erste Psychotherapie-Patientin.', 'Ich war Doktorandin und studierte Klinische Psychologie in Berkeley.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZuDkZrhYJdC",
        "colab_type": "code",
        "outputId": "9ef777bd-d3ed-481f-f106-0d5c7cdad84f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_de = open('test.de', encoding=\"utf8\").read().strip().split('\\n')\n",
        "print(test_de[:2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Mein Ziel in diesem Video und dem nächsten ist es,  ', 'einen Eindruck von der Größe der Erde (wirklich nur der Erde)  ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTyeLVgpyEu8",
        "colab_type": "text"
      },
      "source": [
        "## Translate and write to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZgn-Z2sEcC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !rm \"dev.out.en\"\n",
        "\n",
        "out_file = 'dev.out.en'\n",
        "# out_file = 'test.en'\n",
        "in_sentences = dev_de\n",
        "# in_sentences = test_de\n",
        "\n",
        "\n",
        "bsz = 120\n",
        "model.eval()\n",
        "with open(out_file, 'w') as f:\n",
        "    for i in range(0, len(in_sentences), bsz):\n",
        "        tr_start_time = time.time()\n",
        "        translation = translate(model, in_sentences[i : i + bsz], nlp_de.tokenizer, vocab_de, idx_to_word_en, MAX_LEN, greedy=False)\n",
        "        print(i, time.time() - tr_start_time, translation[0])\n",
        "        for line in translation:\n",
        "            f.write(line + '\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L9M6fQ6yNmC",
        "colab_type": "text"
      },
      "source": [
        "## Calculate BLEU scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdkahcxP9c-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "#!/bin/bash\n",
        "\n",
        "rm \"dev.out.en.detok\"\n",
        "\n",
        "# This is a reference to the gold translations from the dev set\n",
        "REFERENCE_FILE=\"dev.en\"\n",
        "\n",
        "# XXX: Change the following line to point to your model's output!\n",
        "TRANSLATED_FILE=\"dev.out.en\"\n",
        "\n",
        "# The model output is expected to be in a tokenized form. Note, that if you\n",
        "# tokenized your inputs to the model, then simply joined each output token with\n",
        "# whitespace you should get tokenized outputs from your model.\n",
        "# i.e. each output token is separate by whitespace\n",
        "# e.g. \"My model 's output is interesting .\"\n",
        "perl \"mosesdecoder/scripts/tokenizer/detokenizer.perl\" -l en < \"$TRANSLATED_FILE\" > \"$TRANSLATED_FILE.detok\"\n",
        "\n",
        "PARAMS=(\"-tok\" \"intl\" \"-l\" \"de-en\" \"$REFERENCE_FILE\")\n",
        "sacrebleu \"${PARAMS[@]}\" < \"$TRANSLATED_FILE.detok\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}